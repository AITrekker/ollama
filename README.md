# ğŸš€ CAP fhL 2025: Hands-On AI â€“ Running and Building with Local LLMs  

## ğŸ”¹ Overview  
AI isnâ€™t just for research teamsâ€”itâ€™s for every developer. This **hands-on session** will help **developers, managers, and product owners** learn how to:  

âœ… **Run multiple local LLMs**  
âœ… **Compare AI model responses**  
âœ… **Integrate AI into real-world applications**  

---

## ğŸ¯ What Youâ€™ll Learn  

âœ”ï¸ **Set up & run local LLMs** using [Ollama](https://ollama.ai)  
âœ”ï¸ **Compare model responses** to understand their strengths & weaknesses  
âœ”ï¸ **Write scripts** for theming, summarization, & AI-aware features  
âœ”ï¸ **Explore agentic scenarios** â€“ no prior AI expertise required!  

---

## ğŸ› ï¸ Prerequisites  

To **get the most out of this session**, please install the following **beforehand**:  

### **Required Software**  

| Software | Download Link |
|----------|--------------|
| **Ollama** | [Download](https://ollama.ai/download) |
| **Python 3.10+** | [Windows](https://www.python.org/downloads/windows/) \| [Mac](https://www.python.org/downloads/mac-osx/) |

âš¡ **Pro Tip:** Install as **admin** and update `PATH`!  

<img src="https://github.com/user-attachments/assets/a44cefb7-dfde-43ff-af6b-03dd425e12a9" width="400" />

---


# ğŸ—ï¸ Hands-On Exercises  

## ğŸ **Exercise 1: Running Your First Local LLM**  

ğŸ”¹ **Goal:** Download and run your first model, and interact with an LLM using the command line.  

### **1ï¸âƒ£ Pull & Run Your First Model**  
```sh
ollama run llama3.2:1b
```

### **2ï¸âƒ£ Interact with the Model in the Terminal**  
Ask a question, such as "Is hotdog a sandwich?"

Be creative and ask it questions or chat with it.

You now have the powers of a scaled down chatgpt running on your local computer.

ğŸ’¡ **Tip:** Input `/bye` to exit the chat.

---

## ğŸ’¬ **Exercise 2: Build a Simple Web-Based Chatbot**  

ğŸ”¹ **Goal:** Create a basic **web interface** to interact with an LLM.  

### ğŸ“Œ What Youâ€™ll Do:  
âœ… Set up a **simple HTML & JavaScript** front-end  
âœ… Connect your chatbot to **Ollama's API**  

### **1ï¸âƒ£ Create a Working Directory**  
```sh
mkdir llm
cd llm
```

### **2ï¸âƒ£ Download the Chatbot Web Page**  
Download **[single-llm.html](https://github.com/whizamit/llm/blob/main/single-llm.html)** into the `llm` folder.  

### **3ï¸âƒ£ Start a Temporary Web Server**  
| OS | Command |
|----|---------|
| **Windows** | `python -m http.server 8000` |
| **Mac** | `python3 -m http.server 8000` |

### **4ï¸âƒ£ Open in Your Browser**  
Go to **http://localhost:8000/single-llm.html** in your web browser.  

---

## ğŸ”„ **Exercise 3: Compare Multiple LLMs in a Chatbot**  

ğŸ”¹ **Goal:** Build a **more advanced chatbot** that compares multiple LLM responses side by side.  

ğŸ“Œ **What Youâ€™ll Do:**  
âœ… Query multiple LLMs **at once**  
âœ… Display responses **side by side**  
âœ… Analyze **differences in model outputs**  

---

### **1ï¸âƒ£ Understanding LLM Sizes & System Limitations**  

ğŸ’¡ **Not all LLMs are the same size!** Some require more memory and computing power, affecting performance.  

ğŸ”¹ **Windows Users:**  
- Windows laptops **lack unified memory** (which Apple Silicon uses to share RAM between CPU & GPU).  
- Running **larger models (7B-14B)** can be **slow or impossible** due to memory limits.  
- **Solution:** Use **smaller models (1B-3B)** that require less RAM.  

ğŸ”¹ **Mac Users:**  
- Macs with **M1/M2/M3 chips** can run **larger models** due to **unified memory**.  
- If you have a Mac with **16GB+ RAM**, you can try **larger models (7B-14B)** for better results.  

---

### ğŸ§  **Choosing the Right Model**  

#### ğŸ”¹ **Smaller, Lightweight Models (Best for Windows & Low-Memory Macs) -- Recommended**  

| Model ID | Run Command |
|----------|------------|
| `llama3.2:1b` | `ollama run llama3.2:1b` |
| `phi3` | `ollama run phi3:3.8b` |
| `gemma2:2b` | `ollama run gemma2:2b` |
| `deepseek-r1:1.5b` | `ollama run deepseek-r1:1.5b` |

#### ğŸ”¹ **Larger, More Powerful Models (Best for Macs with 16GB+ RAM)**  

| Model ID | Run Command |
|----------|------------|
| `llama3.2:latest` | `ollama run llama3.2:latest` |
| `phi4` | `ollama run phi4:14b` |
| `gemma2:9b` | `ollama run gemma2:9b` |
| `deepseek-r1` | `ollama run deepseek-r1:7b` |

---

### **2ï¸âƒ£ Download the Multiple Chatbot Web Page**  
Download **[multiple-llm.html](https://github.com/whizamit/llm/blob/main/multiple-llm.html)** into the `llm` folder.  

### **3ï¸âƒ£ Open in Your Browser**  
Go to **http://localhost:8000/multiple-llm.html**  

ğŸ’¡ **Tip:** Compare different models on various topics!  

---

## ğŸ“Š **Exercise 4: AI-Powered Thematic Analysis & Summarization**  

ğŸ”¹ **Goal:** Use Python to **analyze themes** and **summarize discussions** from a recent AMA with **Satya Nadella**.  

ğŸ“Œ **What Youâ€™ll Do:**  
âœ… Use AI to **extract key topics**  
âœ… Use AI to **summarize**  

### **1ï¸âƒ£ Download the Multiple Chatbot Web Page**  
Download the following into the `llm` folder.  
[summarize.py](https://github.com/whizamit/llm/blob/main/summarize.py)
| 
[sample.csv](https://github.com/whizamit/llm/blob/main/sample.csv)

### **2ï¸âƒ£ Run the script**  

(Windows) `python summarize.py sample.csv`
(Mac) `python3 summarize.py sample.csv`

### **3ï¸âƒ£ View the output**  
Open the file 'output.csv' to see the summary and theme of each sample question.

---
---

## ğŸ“š Resources  

ğŸ“– [Ollama Models](https://ollama.com/search)  
ğŸ [Python Basics for AI](https://docs.python.org/3/tutorial/)  
ğŸ¤– [Introduction to LLMs by Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuW9U8-vZ_s_cjKPT_FqRStI) 


### âœ¨ Cleanup
Remove all local llms 

(Windows) `ollama list | ForEach-Object { ollama rm $_.Split(" ")[0] }`

(Mac) `ollama list | awk '{print $1}' | xargs -I {} ollama rm {}`
## ğŸ“© Contact  

ğŸ’¡ **Questions?** Open an issue in the GitHub repository or reach out via **Teams** @amigupta, @sasitara


